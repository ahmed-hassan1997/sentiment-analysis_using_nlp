{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "lstm-sentiment-analysis-keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedhassan97/sentiment-analysis_using_nlp/blob/master/lstm_sentiment_analysis_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZjytludoKVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzUZSGr8yxcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucmLLDJHuBzy",
        "colab_type": "text"
      },
      "source": [
        "** ** author : ahmed hassan **\n",
        "   \n",
        " githup : https://github.com/ahmedhassan97\n",
        "\n",
        " linkedin : https://www.kaggle.com/ahmedhassan97\n",
        "\n",
        "project to classify Twitter Sentiment into \n",
        "\n",
        "two classification using nlp algorithm\n",
        "\n",
        "data from kaggle \n",
        "\n",
        "link : https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8FqbQvHt-u6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhd_0QvCp7br",
        "colab_type": "code",
        "outputId": "4c3cc2a9-5c6e-422a-b818-dc3dc4c26a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#zip file\n",
        "from zipfile import ZipFile\n",
        "file_name = \"data.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('done')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e",
        "_uuid": "9b520acffb5cd85d0e1ada968ad0f12cee33a4b5",
        "id": "yrlPN1EJnqy7",
        "colab_type": "text"
      },
      "source": [
        "Only keeping the necessary columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a",
        "_uuid": "d2bc3bbd2ea3961c49e6673145a0a7226c160e58",
        "trusted": false,
        "id": "tMV9XJXGnqy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('Sentiment.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4c0ec63b-cdf8-8e29-812b-0fbbfcea2929",
        "_uuid": "ff12d183224670f9c4c96fd24581b9924d4dff20",
        "id": "jIaLbgs9nqzF",
        "colab_type": "text"
      },
      "source": [
        "Next, I am dropping the 'Neutral' sentiments as my goal was to only differentiate positive and negative tweets. After that, I am filtering the tweets so only valid texts and words remain.  Then, I define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
        "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813",
        "trusted": false,
        "id": "sXESCv8enqzH",
        "colab_type": "code",
        "outputId": "eca7ec33-828c-4128-8507-7e84ef4bdbca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "data = data[data.sentiment != \"Neutral\"]\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
        "\n",
        "print(data[ data['sentiment'] == 'Positive'].size)\n",
        "print(data[ data['sentiment'] == 'Negative'].size)\n",
        "print(data[\"text\"])\n",
        "for idx,row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt',' ')\n",
        "    \n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "print(X[:3])\n",
        "X = pad_sequences(X)\n",
        "print(X)\n",
        "print(X.shape[1])\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4472\n",
            "16986\n",
            "1          scottwalker didnt catch the full gopdebate l...\n",
            "3          robgeorge that carly fiorina is trending  ho...\n",
            "4          danscavino gopdebate w realdonaldtrump deliv...\n",
            "5          gregabbott_tx tedcruz on my first day i will...\n",
            "6          warriorwoman91 i liked her and was happy whe...\n",
            "                               ...                        \n",
            "13866      cappy_yarbrough love to see men who will nev...\n",
            "13867      georgehenryw who thought huckabee exceeded t...\n",
            "13868      lrihendry tedcruz as president i will always...\n",
            "13869      jrehling gopdebate donald trump says that he...\n",
            "13870      lrihendry tedcruz headed into the presidenti...\n",
            "Name: text, Length: 10729, dtype: object\n",
            "[[363, 122, 1, 703, 2, 39, 58, 237, 37, 210, 6, 174, 1761, 12, 1324, 1409, 743], [16, 284, 252, 5, 821, 102, 167, 26, 136, 6, 1, 173, 12, 2, 233, 724, 17], [1261, 2, 303, 23, 1943, 1, 1632, 216, 12, 1, 704, 6, 185, 207, 371, 670]]\n",
            "[[   0    0    0 ... 1324 1409  743]\n",
            " [   0    0    0 ...  233  724   17]\n",
            " [   0    0    0 ...  207  371  670]\n",
            " ...\n",
            " [   0    0    0 ...   72   65    3]\n",
            " [   0    0    0 ... 1022 1423   74]\n",
            " [   0    0    0 ...  197    3  723]]\n",
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9753421e-1303-77d5-b17f-5f25fa08c452",
        "_uuid": "aa7d103e946e631133d86ef3adc73e1a8b1a1e89",
        "id": "T8Mi9bwCnqzS",
        "colab_type": "text"
      },
      "source": [
        "Next, I compose the LSTM Network. Note that **embed_dim**, **lstm_out**, **batch_size**, **droupout_x** variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note that I am using softmax as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
        "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
        "trusted": false,
        "id": "cZvyL3LlnqzU",
        "colab_type": "code",
        "outputId": "50a9076b-41de-4bfb-aead-1a5bd7f8df1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 28, 128)           256000    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_2 (Spatial (None, 28, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 394       \n",
            "=================================================================\n",
            "Total params: 511,194\n",
            "Trainable params: 511,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "15f4ee61-47e4-88c4-4b81-98a85237333f",
        "_uuid": "2dae0f3b95a4ba533453c512e573560a8358e162",
        "id": "zJ6Imd_bnqzd",
        "colab_type": "text"
      },
      "source": [
        "Hereby I declare the train and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0",
        "_uuid": "a380bbfae2d098d407b138fc44622c9913a31c07",
        "trusted": false,
        "id": "-hWmKpXSnqze",
        "colab_type": "code",
        "outputId": "65712d91-ddb9-483c-9802-9917de8867a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "Y = pd.get_dummies(data['sentiment']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7188, 28) (7188, 2)\n",
            "(3541, 28) (3541, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hsPtUHuwDYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "2dd2d42e-67df-4704-9ddd-1dafda2e9dce"
      },
      "source": [
        "print(X_train[:3])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0    0    0    0    0    0   72   11   80  312    7  980\n",
            "   781  758  101   28   99  234  278   21  375  199  414    6    7  185]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0   10   86\n",
            "    15    7  345  104  182   29  359  125   63   15  354   13    2    3]\n",
            " [   0    0    0    0    0    0    0    0    0  422  553   91   19  414\n",
            "    62   25  431 1036   83   40  673  508 1734  816  264  312   16  134]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6",
        "_uuid": "8799a667a2c0254cb367c193d86e07ee36d91dd7",
        "id": "5bhjrAnmnqzo",
        "colab_type": "text"
      },
      "source": [
        "Here we train the Network. We should run much more than 7 epoch, but I would have to wait forever for kaggle, so it is 7 for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
        "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7",
        "trusted": false,
        "id": "BM7cJeu9nqzr",
        "colab_type": "code",
        "outputId": "ea33b684-6e24-44ab-aed0-778d99d0dd58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "batch_size = 32\n",
        "model.fit(X_train, Y_train, epochs = 9, batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            " - 20s - loss: 0.4405 - accuracy: 0.8144\n",
            "Epoch 2/9\n",
            " - 20s - loss: 0.3251 - accuracy: 0.8639\n",
            "Epoch 3/9\n",
            " - 20s - loss: 0.2833 - accuracy: 0.8830\n",
            "Epoch 4/9\n",
            " - 20s - loss: 0.2516 - accuracy: 0.9001\n",
            "Epoch 5/9\n",
            " - 20s - loss: 0.2272 - accuracy: 0.9065\n",
            "Epoch 6/9\n",
            " - 20s - loss: 0.2074 - accuracy: 0.9174\n",
            "Epoch 7/9\n",
            " - 20s - loss: 0.1855 - accuracy: 0.9260\n",
            "Epoch 8/9\n",
            " - 20s - loss: 0.1692 - accuracy: 0.9329\n",
            "Epoch 9/9\n",
            " - 20s - loss: 0.1622 - accuracy: 0.9359\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fd696a8bcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434",
        "_uuid": "47e99d7ed1f27a85eb01dbafc71b66b329fb1d12",
        "id": "zEiYB-fbnqz0",
        "colab_type": "text"
      },
      "source": [
        "Extracting a validation set, and measuring score and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "a970f412-722f-6d6d-72c8-325d0901ccef",
        "_uuid": "7872f6ea819a5d4d08394ba6db8436f9cb2cfe1c",
        "trusted": false,
        "id": "lvl__4gynqz2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7605d7da-0df8-40f2-f001-c16f35960af6"
      },
      "source": [
        "validation_size = 1500\n",
        "\n",
        "X_validate = X_test[-validation_size:]\n",
        "Y_validate = Y_test[-validation_size:]\n",
        "X_test = X_test[:-validation_size]\n",
        "Y_test = Y_test[:-validation_size]\n",
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score: 0.48\n",
            "acc: 0.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "018ebf39-9414-27d0-232c-a34de051feaf",
        "_uuid": "4b54f18bbf22a953c60f271c318cb076e684df9c",
        "id": "9W6fec3vnq0A",
        "colab_type": "text"
      },
      "source": [
        "Finally measuring the number of correct guesses.  It is clear that finding negative tweets goes very well for the Network but deciding whether is positive is not really. My educated guess here is that the positive training set is dramatically smaller than the negative, hence the \"bad\" results for positive tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1add73e9-c6fb-7e4c-8715-ea92f519d2a6",
        "_uuid": "f80e9f3cf281adb3ab0357cbf6f886eb1dce3005",
        "trusted": false,
        "id": "Ot0LIvDunq0B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e4ed21af-b710-43c5-c594-fad45727bcab"
      },
      "source": [
        "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "for x in range(len(X_validate)):\n",
        "    \n",
        "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
        "   \n",
        "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
        "        if np.argmax(Y_validate[x]) == 0:\n",
        "            neg_correct += 1\n",
        "        else:\n",
        "            pos_correct += 1\n",
        "       \n",
        "    if np.argmax(Y_validate[x]) == 0:\n",
        "        neg_cnt += 1\n",
        "    else:\n",
        "        pos_cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
        "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_acc 58.89967637540453 %\n",
            "neg_acc 90.76406381192275 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "890a03c9-316e-4d55-98e1-ba29045eff6c",
        "_uuid": "cfcbefe939b72297019e221ca3f5a283974bffff",
        "id": "th9DPB0Fnq0J",
        "colab_type": "text"
      },
      "source": [
        "As it was requested by the crowd, I extended the kernel with a prediction example, and also updated the API calls to Keras 2.0. Please note that the network performs poorly. Its because the training data is very unbalanced (pos: 4472, neg: 16986), you should get more data, use other dataset, use pre-trained model, or weight classes to achieve reliable predictions.\n",
        "\n",
        "I have created this kernel when I knew much less about LSTM & ML. It is a really basic, beginner level kernel, yet it had a huge audience in the past year. I had a lot of private questions and requests regarding this notebook and I tried my best to help and answer them . In the future I am not planning to answer custom questions and support/enhance this kernel in any ways. Thank you my folks :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd",
        "_uuid": "d9aac68e2013b3beffb6a764cc5b85be83073e66",
        "trusted": false,
        "id": "Qp1H_7nZnq0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d8e5f632-abc1-498c-ed77-1fe6ca2c4287"
      },
      "source": [
        "twt = [' i am very sad ']\n",
        "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
        "twt = tokenizer.texts_to_sequences(twt)\n",
        "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
        "twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
        "print(twt)\n",
        "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
        "if(np.argmax(sentiment) == 0):\n",
        "    print(\"negative\")\n",
        "elif (np.argmax(sentiment) == 1):\n",
        "    print(\"positive\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   9 158 100 725]]\n",
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c611b55c-92e4-4a33-8e82-1812bef6193d",
        "_uuid": "8b10995b0832ec98ba0c75832186fcb09b1a2d5f",
        "trusted": false,
        "id": "vjXw6Riwnq0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}